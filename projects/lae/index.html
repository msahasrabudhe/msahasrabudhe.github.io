<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Lifting Autoencoders</title>
    <meta name="description" content="Project Page for Lifting Autoencoders">
    <meta name="author" content="Mihir Sahasrabudhe">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href="./style_files/bootstrap.min.css" type="text/css" rel="stylesheet">
    <link href="./style_files/custom.css" type="text/css" rel="stylesheet">
</head>

<body>
    <link rel="stylesheet" type="text/css" href="./lae.css" media="screen">
    <h1>
        Lifting Autoencoders: Unsupervised Learning of a Fully-Disentangled 3D Morphable Model using Deep Non-Rigid Structure from Motion
    </h1>

    <table>
        <tbody>
            <tr>
                <td><a href="https://msahasrabudhe.github.io/">Mihir Sahasrabudhe*</a>
                    <span class="super">1</span>
                </td>
                <td><a href="https://zhixinshu.github.io/">Zhixin Shu*</a>
                    <span class="super">2</span>
                </td>
                <td><a href="https://www.turing.ac.uk/people/doctoral-students/edward-bartrum">Edward Bartrum</a>
                    <span class="super">3,4</span>
                </td>
                <td><a href="http://alpguler.com/">Rıza Alp Güler</a>
                    <span class="super">5,6</span>
                </td>
                <td><a href="http://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                    <span class="super">2</span>
                </td>
                <td><a href="http://www0.cs.ucl.ac.uk/staff/I.Kokkinos/">Iasonas Kokkinos</a>
                    <span class="super">4,6</span>
                </td>
            </tr>
            <!-- email addresses are commented as they look not good. But you can find them here:) -->
            <!-- <tr>
            <td>kemmaATcsDOTstonybrookDOTedu</td>
            <td>minhhuaiATcsDOTstonybrookDOTedu</td>
            <td>samarasATcsDOTstonybrookDOTedu</td>
        </tr> -->

            <tr>
                <td colspan="6">
                    <sup>1</sup>CentraleSup&eacute;lec, 
                    <sup>2</sup>Stony Brook University, 
                    <sup>3</sup>The Alan Turing Institute, 
                    <sup>4</sup>University College London, 
                    <sup>5</sup>Imperial College, 
                    <sup>6</sup>Ariel AI
                </td>
            </tr>

<!--
            <tr>
                &nbsp;<br />
                <td colspan="1">
                    CentraleSup&eacute;lec
                </td>
                <td colspan="1">
                    Stony Brook University
                </td>
                <td colspan="1">
                    The Alan Turing Institute,<br />
                    University College London
                </td>
                <td colspan="1">
                    Ariel AI, <br />
                    Imperial College
                </td>
                <td colspan="1">
                    Stony Brook University
                </td>
                <td colspan="1">
                    Ariel AI,<br />
                    University College London
                </td>
            </tr>

-->

        </tbody>
    </table>


    <h2>
        Abstract
    </h2>
    <div class="text"> In this work we introduce Lifting Autoencoders, a 
    generative 3D surface-based model of object categories. We bring 
    together ideas from non-rigid structure from motion, image formation, 
    and morphable models to learn a controllable, geometric model of 3D 
    categories in an entirely unsupervised manner from an unstructured 
    set of images. We exploit the 3D geometric nature of our model and 
    use normal information to disentangle appearance into illumination, 
    shading and albedo. We further use weak supervision to disentangle 
    the non-rigid shape variability of human faces into identity and 
    expression. We combine the 3D representation with a differentiable 
    renderer to generate RGB images and  append an adversarially 
    trained refinement network  to obtain sharp, photorealistic image 
    reconstruction results. The learned generative model can be 
    controlled in terms of interpretable geometry and appearance 
    factors, allowing us to perform photorealistic image manipulation 
    of identity, expression, 3D  pose, and illumination properties. 
    </div>

    <!--
    <h2>
        Video
    </h2>
    <table id="imgtable">
        <tbody>
            <tr>
                <td class="imgitem">
<iframe width="560" height="315" src="https://www.youtube.com/embed/hwVGD6BYZd0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </td>
            </tr>
    </table>
    -->


    <h2>
        Shape from unconstrained images
    </h2>
    <table id="imgtable">
        <tbody>
            <tr>
                <td class="imgitem">
                    <img src="images/teaser6.png" alt="p1">
                </td>
            </tr>
    </table>

    <h2>
        Network Architectures
    </h2>
    <table id="imgtable">
        <tbody>
            <tr>
                <td class="imgitem">
                    <img src="images/model.png" alt="p1">
                </td>
            </tr>
    </table>

    <h2>
        Learnt shape visualisation from different viewpoints
    </h2>
    <table id="imgtable">
        <tbody>
            <tr>
                <td class="imgitem">
                    <img src="images/rotation.png" alt="p1" width="150">
                </td>
            </tr>
    </table>

    <h2>
        Interpolating 
    </h2>
    <table id="imgtable">
        <tbody>
            <tr>
                <td class="imgitem">
                    <img src="images/all_interpolations.png" alt="p1" width="150">
                </td>
            </tr>
    </table>

    <h2>
        Face Image Manipulation
    </h2>
    <table id="imgtable">
        <tbody>
            <tr>
                <td class="imgitem">
                    <img src="./dae_files/manip.png" alt="p1">
                </td>
            </tr>
    </table>

    <h4>
        Code
    </h4>
    <table id="dltable">
        <tbody>
            <tr>
                <td>
                    <!-- <a href="https://github.com/zhixinshu/DeformingAutoencoders-pytorch">PyTorch Implementation.</a> -->
                    Pytorch Implementation coming soon.
                </td>
            </tr>
        </tbody>
    </table>

    <!--
    <div class="text">
        If using the code, please cite:
        <br>
        <a href="http://www3.cs.stonybrook.edu/~cvl/content/papers/2018/Shu_ECCV18.pdf">Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance, </a>Zhixin Shu, Mihir Sahasrabudhe, Riza Alp Guler, Dimitris Samaras, Nikos Paragios, and Iasonas Kokkinos. European Conference on Computer Vision (ECCV), 2018  [
        <a href="http://www3.cs.stonybrook.edu/~cvl/content/papers/2018/Shu_ECCV18.bib">BibTex</a> ]
    </div>
    -->
    <!--
    <h2>
        Acknowledgements
    </h2>
    <div class="text">
        This work was supported by a gift from Adobe, NSF grants CNS-1718014 and DMS 1737876, the Partner University Fund, and the SUNY2020 Infrastructure Transportation Security Center. Rıza Alp Güler was supported by the European Horizons 2020 grant no 643666 (I-Support).
    </div>
    -->

    <div class="text">
         © All rights reserved.
    </div>



</body>

</html>
